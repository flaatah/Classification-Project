# -*- coding: utf-8 -*-
"""Ara_you_a_Female _or _a _Male? Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15LKMCVsp3oRw_upay7M3AkgFHRWWYGEk
"""

# In this project I tried to predict whether a person living in Saudi Arabia is a Male or a Female  based on the features given 
# I used the basic algorithm for each model since the data is small
# This project is for learning purpose

#import libraries 
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import scipy as sp
from google.colab import files
import io
import warnings
warnings.simplefilter("ignore")

uploaded = files.upload()

df = pd.read_csv(io.BytesIO(uploaded['feat_engineering_salary_by_edu.csv']))

df.head()

#drop Unnamed: 0 
df.drop(['Unnamed: 0'], axis =1, inplace=True)
df

# Dummy variables
df_D = pd.get_dummies(df, columns=['Degree Level', 'Nationality'],drop_first=True)

df_D.head()

X = df_D.drop('Gender',axis =1)
y = pd.get_dummies(df_D, columns=['Gender'],drop_first=True)

y

y = y['Gender_Male']

y

df_D

y

y.unique()

#splitting the data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=100)

#create a funtion to excute training the data and predicting with accuracy scocre  

def run_model(model, scaled_X_train, y_train, scaled_X_test, y_test):

  model.fit(scaled_X_train, y_train)

  pred = model.predict(scaled_X_test)

  print(f'accuracy_score: {accuracy_score}')
  print(accuracy_score(y_test, pred));

  print(f'class_report:{classification_report}')
  print(classification_report(y_test, pred))
  

  print(f'confusion_matrix: {plot_confusion_matrix}')
  plot_confusion_matrix(model, scaled_X_test, y_test)

"""##LogisticRegression"""

#import LogisticRegression
#import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

#scaling X_train and X_test
scaler = StandardScaler()

scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test= scaler.transform(X_test)

log_model = LogisticRegression()

#train the data
log_model.fit(scaled_X_train, y_train)

#import metrics
from sklearn.metrics import accuracy_score,confusion_matrix, classification_report, plot_confusion_matrix

# running the funcion

run_model(log_model, scaled_X_train, y_train, scaled_X_test, y_test)

"""##KNeighborsClassifier"""

from sklearn.neighbors import KNeighborsClassifier

scaler = StandardScaler()

scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test= scaler.transform(X_test)

knn = KNeighborsClassifier()

knn.fit(scaled_X_train, y_train)

run_model(knn, scaled_X_train, y_train, scaled_X_test, y_test)

"""##Support Vector Classification"""

from sklearn.svm import SVC

svc = SVC()

scaler = StandardScaler()

scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test= scaler.transform(X_test)

svc.fit(scaled_X_train, y_train)

run_model(svc, scaled_X_train, y_train, scaled_X_test, y_test)

"""##Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

scaler = StandardScaler()

scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test= scaler.transform(X_test)

dtc = DecisionTreeClassifier()

run_model(dtc, scaled_X_train, y_train, scaled_X_test, y_test)

#importing_features
dtc.feature_importances_

imp_feat = pd.DataFrame(index=X.columns,data=dtc.feature_importances_,columns=['Feature_Importance'])

imp_feat.sort_values('Feature_Importance', ascending=False)

"""##RandomForestClassifier"""

from sklearn.ensemble import RandomForestClassifier

scaler = StandardScaler()

scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test= scaler.transform(X_test)

rfc = RandomForestClassifier()

run_model(rfc, scaled_X_train, y_train, scaled_X_test, y_test)

rfc.feature_importances_

imp_feat_R = pd.DataFrame(index=X.columns,data=rfc.feature_importances_,columns=['Feature_Importance'])

imp_feat_R.sort_values('Feature_Importance', ascending=False)

"""##AdaBoostClassifier"""

from sklearn.ensemble import AdaBoostClassifier

scaler = StandardScaler()

scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test= scaler.transform(X_test)

AdaB = AdaBoostClassifier()

run_model(AdaB, scaled_X_train, y_train, scaled_X_test, y_test)

AdaB.feature_importances_

imp_feat_AdaB = pd.DataFrame(index=X.columns,data=AdaB.feature_importances_,columns=['Feature_Importance'])

imp_feat_AdaB.sort_values('Feature_Importance', ascending=False)

"""##GradientBoostingClassifier"""

from sklearn.ensemble import GradientBoostingClassifier

scaler = StandardScaler()

scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test= scaler.transform(X_test)

Gd = GradientBoostingClassifier()

run_model(Gd, scaled_X_train, y_train, scaled_X_test, y_test)

Gd.feature_importances_

imp_feat_Gd = pd.DataFrame(index=X.columns,data=Gd.feature_importances_,columns=['Feature_Importance'])

imp_feat_Gd.sort_values('Feature_Importance', ascending=False)

# Based from Machine Learming Algorithms resuilts it looks  RandomForestClassifier perfome better 
# with 0.90 accuracy

